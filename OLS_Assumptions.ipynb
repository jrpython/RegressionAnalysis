{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumtions for OLS\n",
    "Source: https://365datascience.com/ols-assumptions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <u>First OLS Assumption</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is linearity. It is called a <b>linear regression</b>. As you may know, there are other types of regressions with more sophisticated models. The linear regression is the simplest one and assumes linearity. <b>Each independent variable is multiplied by a coefficient and summed up to predict the value of the dependent variable.</b>\n",
    "\n",
    "How can you verify if the relationship between two variables is linear? The easiest way is to choose an independent variable X1 and plot it against the depended Y on a scatter plot. If the data points form a pattern that looks like a straight line, then a linear regression model is suitable.\n",
    "\n",
    "<img src=\"images/ols_assumption1.jpg\">\n",
    "\n",
    "<b>An Example Where There is No Linearity</b><br>\n",
    "Let’s see a case where this OLS assumption is violated. We can plot another variable X2 against Y on a scatter plot. As you can see in the picture below, there is no straight line that fits the data well.\n",
    "\n",
    "Actually, a curved line would be a very good fit. <b>Using a linear regression would not be appropriate.</b>\n",
    "\n",
    "<img src=\"images/ols_assumption1_violated.jpg\">\n",
    "\n",
    "<b>Fixes for Linearity</b>\n",
    "\n",
    "Linearity seems restrictive, but there are easy fixes for it. You can run a non-linear regression or transform your relationship. There are exponential and logarithmical transformations that help with that. The quadratic relationship we saw before, could be easily transformed into a straight line with the appropriate methods.\n",
    "\n",
    "<b>Important:</b> The takeaway is, if the relationship is nonlinear, you should not use the data before transforming it appropriately.\n",
    "\n",
    "<img src=\"images/ols_assumption1_fix.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Second OLS Assumption</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is <b>endogeneity</b> of regressors. Mathematically, this is expressed as the covariance of the error and the Xs is 0 for any error or x. It refers to the prohibition of a link between the independent variables and the errors, mathematically expressed in the following way.\n",
    "\n",
    "<img src=\"images/ols_assumption2.jpg\">\n",
    "\n",
    "<b>An Example Where There is No Endogeneity:</b><br>\n",
    "\n",
    "Think about it. The error is the difference between the observed values and the predicted values. In this case, it is correlated with our independent values.\n",
    "\n",
    "<img src=\"images/ols_assumption2_violated.jpg\">\n",
    "\n",
    "<b>Omitted Variable Bias:</b>\n",
    "\n",
    "This is a problem referred to as omitted variable bias. Omitted variable bias is introduced to the model when you forget to include a relevant variable.\n",
    "\n",
    "As each independent variable explains y, they move together and are somewhat correlated. Similarly, y is also explained by the omitted variable, so they are also correlated. Chances are, the omitted variable is also correlated with at least one independent x. However, you forgot to include it as a regressor.\n",
    "\n",
    " \n",
    "\n",
    "Everything that you don’t explain with your model goes into the error. So, actually, the error becomes correlated with everything else.\n",
    "\n",
    "<img src=\"images/ols_assumption2_Omitted_Variable_Bias.jpg\">\n",
    "\n",
    "<b>Fixing the Problem:</b>\n",
    "\n",
    "<b>Omitted variable bias is hard to fix.</b> Think of all the things you may have missed that led to this poor result. We have only one variable but when your model is exhaustive with 10 variables or more, you may feel disheartened.\n",
    "\n",
    "Critical thinking time. Where did we draw the sample from? Can we get a better sample? Why is bigger real estate cheaper?\n",
    "\n",
    "What’s the bottom line? Omitted variable bias is a pain in the neck.\n",
    "\n",
    "* It is always different\n",
    "* Always sneaky\n",
    "* Only experience and advanced knowledge on the subject can help.\n",
    "* Always check for it and if you can’t think of anything, ask a colleague for assistance!\n",
    "\n",
    "<b>Important:</b> The incorrect exclusion of a variable can lead to biased and counterintuitive estimates that are toxic to our regression analysis. Also, an incorrect inclusion of a variable leads to inefficient estimates. They don’t bias the regression, so you can immediately drop them. When in doubt, just include the variables and try your luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Third OLS Assumption</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third OLS assumtion comprises of three parts:\n",
    "* normality\n",
    "* zero mean\n",
    "* and homoscedasticity<br>\n",
    "\n",
    "of the error term.\n",
    "\n",
    "<b>Normality:</b>\n",
    "The first one is easy. We assume the error term is normally distributed.\n",
    "\n",
    "Normal distribution is not required for creating the regression but for making inferences. All regression tables are full of t-statistics and F-statistics.\n",
    "\n",
    "These things work because we assume normality of the error term. What should we do if the error term is <b>not normally distributed?</b> The central limit theorem will do the job. For large samples, the central limit theorem applies for the error terms too. Therefore, we can consider normality as a given for us.\n",
    "\n",
    "<img src=\"images/ols_assumption3_1.jpg\">\n",
    "\n",
    "<b>Zero Mean:</b>\n",
    "What about a zero mean of error terms? Well, if the mean is not expected to be zero, then the line is not the best fitting one. However, having an intercept solves that problem, so in real-life it is unusual to violate this part of the assumption.\n",
    "\n",
    "<img src=\"images/ols_assumption3_2.jpg\">\n",
    "\n",
    "<b>Homoscedasticity:</b>\n",
    "\n",
    "Homoscedasticity means to have equal variance. So, the error terms should have equal variance one with the other.\n",
    "\n",
    "What if there was a pattern in the variance?\n",
    "\n",
    "Well, an example of a dataset, where errors have a different variance, looks like this:\n",
    "\n",
    "<img src=\"images/ols_assumption3_2.1.jpg\">\n",
    "\n",
    "It starts close to the regression line and goes further away. This would imply that, for smaller values of the independent and dependent variables, we would have a better prediction than for bigger values. And as you might have guessed, we really don’t like this uncertainty.\n",
    "\n",
    "<b>Preventing Heteroscedasticity</b>\n",
    "\n",
    "There is a way to circumvent heteroscedasticity.\n",
    "\n",
    "* First, we should check for omitted variable bias – that’s always an idea.\n",
    "* After that, we can look for outliers and try to remove them.\n",
    "* Finally, we shouldn’t forget about a statistician’s best friend – the log- transformation.\n",
    "\n",
    "Naturally, log stands for a logarithm. You can change the scale of the graph to a log scale. For each observation in the dependent variable, calculate its natural log and then create a regression between the log of y and the independent Xs.\n",
    "\n",
    "Conversely, you can take the independent X that is causing you trouble and do the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Fourth OLS Assumption</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penultimate OLS assumption is the <b>no autocorrelation assumption</b>. It is also known as <b>no serial correlation.</b> Unfortunately, it cannot be relaxed.\n",
    "\n",
    "Mathematically, it looks like this: errors are assumed to be uncorrelated.\n",
    "\n",
    "<img src=\"images/ols_assumption4.jpg\">\n",
    "\n",
    "Where can we observe serial correlation between errors? It is highly unlikely to find it in data taken at one moment of time, known as cross-sectional data. However, it is very common in time series data.\n",
    "\n",
    "<b>How to Detect Autocorrelation</b>\n",
    "\n",
    "A common way is to plot all the residuals on a graph and look for patterns. If you can’t find any, you’re safe.\n",
    "\n",
    "<img src=\"images/ols_assumption4_detection.jpg\">\n",
    "\n",
    "Another is the Durbin-Watson test which you have in the summary for the table provided by ‘statsmodels’.\n",
    "\n",
    "Generally, its value falls between 0 and 4.\n",
    "\n",
    "2 indicates no autocorrelation.\n",
    "\n",
    "Whereas, values below 1 (<1) and above 3 (>3) are a cause for alarm.\n",
    "\n",
    "<img src=\"images/ols_assumption4_detection_2.jpg\">\n",
    "\n",
    "<b>The Remedy</b>\n",
    "\n",
    "But, what’s the remedy you may ask? Unfortunately, there is no remedy. As we mentioned before, we cannot relax this OLS assumption. The only thing we can do is avoid using a linear regression in such a setting.\n",
    "\n",
    "There are other types of regressions that deal with time series data. It is possible to use an autoregressive model, a moving average model, or even an autoregressive moving average model. There’s also an autoregressive integrated moving average model.\n",
    "\n",
    "Make your choice as you will, but don’t use the linear regression model when error terms are autocorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Fifth OLS Assumption</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last OLS assumption is <b>no multicollinearity.</b> We observe multicollinearity when two or more variables have a high correlation.\n",
    "\n",
    "a and b are two variables with an exact linear combination. a can be represented using b, and b can be represented using a. In a model containing a and b, we would have perfect multicollinearity.\n",
    "\n",
    "This imposes a big problem to our regression model as the coefficients will be wrongly estimated. The reasoning is that, if a can be represented using b, there is no point using both. We can just keep one of them.\n",
    "\n",
    "<img src=\"images/ols_assumption5.jpg\">\n",
    "\n",
    "<b>Providing a Case in Point</b>\n",
    "\n",
    "Another example would be two variables c and d with a correlation of 90%. If we had a regression model using c and d, we would also have multicollinearity, although not perfect. Here, the assumption is still violated and poses a problem to our model.\n",
    "\n",
    "<img src=\"images/ols_assumption5_2.jpg\">\n",
    "\n",
    "<b>How to Fix it</b>\n",
    "\n",
    "There are three types of fixes:\n",
    "\n",
    "* The first one is to drop one of the two variables.\n",
    "* The second is to transform them into one variable.\n",
    "* The third possibility is tricky. If you are super confident in your skills, you can keep them both, while treating them with extreme caution.\n",
    "\n",
    "The correct approach depends on the research at hand.\n",
    "\n",
    "Multicollinearity is a big problem but is also the easiest to notice. Before creating the regression, find the correlation between each two pairs of independent variables. After doing that, you will know if a multicollinearity problem may arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
